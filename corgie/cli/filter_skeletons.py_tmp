import click
from cloudfiles import CloudFiles
from cloudvolume import PrecomputedSkeleton, Skeleton
import numpy as np
import os
import pickle

from corgie import scheduling, argparsers, stack
from corgie.log import logger as corgie_logger
from corgie.boundingcube import get_bcube_from_vertices
from corgie.argparsers import (
    LAYER_HELP_STR,
    corgie_optgroup,
    corgie_option,
    create_stack_from_spec,
)


def get_skeleton(src_path, skeleton_id_str):
    cf = CloudFiles(src_path)
    return Skeleton.from_precomputed(cf.get(skeleton_id_str))


class GenerateNewSkeletonTask(scheduling.Task):
    def __init__(
        self, skeleton_id_str, src_path, dst_path, task_vertex_size, vertex_sort=False
    ):
        self.src_path = src_path
        self.dst_path = dst_path
        self.skeleton_id_str = skeleton_id_str
        self.task_vertex_size = task_vertex_size
        self.vertex_sort = vertex_sort
        super().__init__()

    def execute(self):
        skeleton = get_skeleton(self.src_path, self.skeleton_id_str)
        if self.vertex_sort:
            vertex_sort = skeleton.vertices[:, 2].argsort()
        else:
            vertex_sort = np.arange(0, len(skeleton.vertices))
        number_vertices = len(skeleton.vertices)
        index_points = list(range(0, number_vertices, self.task_vertex_size))
        cf = CloudFiles(f"{self.dst_path}")
        array_filenames = []
        for i in range(len(index_points)):
            start_index = index_points[i]
            if i + 1 == len(index_points):
                end_index = number_vertices
            else:
                end_index = index_points[i+1]
            array_filenames.append(
                f"intermediary_arrays/{self.skeleton_id_str}:{start_index}-{end_index}"
            )
        array_files = cf.get(array_filenames)
        # Dict to make sure arrays are concatenated in correct order
        array_dict = {}
        for array_file in array_files:
            array_dict[array_file["path"]] = pickle.loads(array_file["content"])
        array_arrays = []
        for array_filename in array_filenames:
            array_arrays.append(array_dict[array_filename])
        array_arrays = np.concatenate(array_arrays)
        # Restore the correct order of the vertices
        restore_sort = vertex_sort.argsort()
        new_vertices = array_arrays[restore_sort]
        new_skeleton = Skeleton(
            vertices=new_vertices,
            edges=skeleton.edges,
            radii=skeleton.radius,
            vertex_types=skeleton.vertex_types,
            space=skeleton.space,
            transform=skeleton.transform,
        )
        cf.put(
            path=self.skeleton_id_str,
            content=new_skeleton.to_precomputed(),
            compress="gzip",
        )


class TransformSkeletonVerticesTask(scheduling.Task):
    def __init__(
        self,
        vector_field_layer,
        src_path,
        skeleton_id_str,
        dst_path,
        field_mip,
        start_vertex_index,
        end_vertex_index,
        vertex_sort=False,
    ):
        self.vector_field_layer = vector_field_layer
        self.skeleton_id_str = skeleton_id_str
        self.src_path = src_path
        self.dst_path = dst_path
        self.field_mip = field_mip
        self.start_vertex_index = start_vertex_index
        self.end_vertex_index = end_vertex_index
        self.vertex_sort = vertex_sort
        super().__init__()

    def execute(self):
        skeleton = get_skeleton(self.src_path, self.skeleton_id_str)
        if self.vertex_sort:
            vertex_sort = skeleton.vertices[:, 2].argsort()
        else:
            vertex_sort = np.arange(0, len(skeleton.vertices))
        # How many vertices we will use at once to get a bcube to download from the vector field
        vertex_process_size = 50
        vertices_to_transform = skeleton.vertices[
            vertex_sort[self.start_vertex_index : self.end_vertex_index]
        ]
        index_vertices = list(range(0, self.number_vertices, vertex_process_size))
        new_vertices = []
        for i in range(len(index_vertices)):
            if i + 1 == len(index_vertices):
                current_batch_vertices = vertices_to_transform[index_vertices[i] :]
            else:
                current_batch_vertices = vertices_to_transform[
                    index_vertices[i] : index_vertices[i + 1]
                ]
            field_resolution = np.array(
                self.vector_field_layer.resolution(self.field_mip)
            )
            bcube = get_bcube_from_vertices(
                vertices=current_batch_vertices,
                resolution=field_resolution,
                mip=self.field_mip,
            )
            field_data = self.vector_field_layer.read(
                bcube=bcube, mip=self.field_mip
            ).permute(2,3,0,1)
            current_batch_vertices_to_mip = current_batch_vertices / field_resolution
            bcube_minpt = bcube.minpt(self.field_mip)
            field_indices = current_batch_vertices_to_mip.astype(np.int) - bcube_minpt
            # The magnitude of vectors in vector fields are stored in MIP 0 Resolution
            mip0_pixel_resolution = self.vector_field_layer.resolution(self.field_mip)
            vectors_to_add = []
            for cur_field_index in field_indices:
                vector_at_point = field_data[
                    cur_field_index[0], cur_field_index[1], cur_field_index[2]
                ]
                # Each vector is stored in [Y,X] format
                vectors_to_add.append(
                    [
                        int(mip0_pixel_resolution[0] * vector_at_point[1].item()),
                        int(mip0_pixel_resolution[1] * vector_at_point[0].item()),
                        0,
                    ]
                )
            vectors_to_add = np.array(vectors_to_add)
            # import ipdb
            # ipdb.set_trace()
            current_batch_warped_vertices = current_batch_vertices + vectors_to_add
            new_vertices.append(current_batch_warped_vertices)
        new_vertices = np.concatenate(new_vertices)
        cf = CloudFiles(f"{self.dst_path}/intermediary_arrays/")
        cf.put(
            path=f"{self.skeleton_id_str}:{self.start_vertex_index}-{self.end_vertex_index}",
            content=pickle.dumps(new_vertices),
        )

    @property
    def number_vertices(self):
        return self.end_vertex_index - self.start_vertex_index


def get_skeleton_vert_neighbor_ids(sk, vert_id):
    vert_edges = np.where(
        np.isin(sk.edges[:, 0], vert_id) + \
        np.isin(sk.edges[:, 1], vert_id)\
    )[0]

    neighbors = sk.edges[vert_edges[-1]].flatten()

    # filter out self
    self_index = np.where(neighbors == vert_id)
    neighbors = np.delete(neighbors, self_index)

    return neighbors


class FilterSkeletonsJob(scheduling.Job):
    def __init__(
        self,
        src_path,
        dst_path,
        skeleton_ids,
        skeleton_length_file=None,
    ):
        self.src_path = src_path
        self.dst_path = dst_path

        if skeleton_ids is None:
            self.skeleton_ids = self.get_all_skeleton_ids()
        else:
            self.skeleton_ids = skeleton_ids
        self.skeleton_length_file = skeleton_length_file
        super().__init__()

    def task_generator(self):
        bad_sections = sorted([17891])
        skeletons = self.get_skeletons(self.src_path)

        for skeleton_id_str, sk in skeletons.items():
            # Vector field chunks are typically chunked by 1 in z, so we process
            length_before = sk.cable_length()

            while True:
                verts = sk.vertices
                vert_zs = verts[:, 2].copy()
                vert_zs /= 40 #hack
                vert_zs = vert_zs.astype(np.int32)

                bad_verts = np.where(np.isin(vert_zs, bad_sections))[0]
                print (bad_verts.size)
                if bad_verts.size == 0:
                    break

                # Find the next set of vertices to remove so that
                # we don't mess with indices, remove the in the next pass
                deleted = 0
                for bv in bad_verts:
                    neighbors = get_skeleton_vert_neighbor_ids(sk, bv)

                    # filter out other bad vertices
                    print (bv, neighbors)
                    bad_index = np.where(np.isin(neighbors, bad_verts))
                    neighbors = np.delete(neighbors, bad_index)
                    print (bv, neighbors)

                    assert (np.isin(neighbors.flatten(), bad_verts).sum() == 0)

                    # if there's a good neighbor, reasign all the endpoints to it
                    # else wait until the next round
                    if len(neighbors) != 0:
                        replacement = neighbors[0]
                        assert (replacement not in bad_verts)
                        ed = np.expand_dims(sk.edges, -1)
                        ed[np.where(ed == bv)] = replacement
                        sk.edges = ed.squeeze(-1)
                        # this leaves self-edges
                        print ("Deleted!")
                        deleted += 1

                #old_v_count = sk.vertices.shape[0]
                #old_e_count = sk.edges.shape[0]
                # this removes self-edges
                sk = sk.consolidate()
                new_v_count = sk.vertices.shape[0]
                new_e_count = sk.edges.shape[0]
                #print (deleted)
                #print ("Verts: ", new_v_count, old_v_count)
                #print ("Edges: ", new_e_count, old_e_count)
                #if new_v_count + deleted != old_v_count:
                #    print ("SOmehting is wrong with the count")
                #    import ipdb; ipdb.set_trace()

                #bad_count = 0
                #for v in range(sk.vertices.shape[0]):
                #    z = int(sk.vertices[v][-1] / 40)
                #    if z in bad_sections:
                #        bad_count += 1
        print (length_before, sk.cable_length())
        import ipdb; ipdb.set_trace()

                #print (len(bad_verts))
                #print (sk.vertices.shape)


    def get_skeletons(self, folder):
        skeleton_filenames = [str(skeleton_id) for skeleton_id in self.skeleton_ids]
        cf = CloudFiles(folder)
        skeleton_files = cf.get(skeleton_filenames)
        skeletons = {}
        for skeleton_file in skeleton_files:
            skeleton_id_str = skeleton_file["path"]
            skeleton = Skeleton.from_precomputed(skeleton_file["content"])
            skeletons[skeleton_id_str] = skeleton
        return skeletons

    def get_all_skeleton_ids(self):
        cf = CloudFiles(self.src_path)
        skeleton_filenames = cf.list(flat=True)
        skeleton_ids = []
        for skeleton_filename in skeleton_filenames:
            if ":" in skeleton_filename:
                # Fragment
                continue
            skeleton_ids.append(int(skeleton_filename))
        return skeleton_ids


@click.command()
@corgie_optgroup("Layer Parameters")
@corgie_option(
    "--src_folder",
    nargs=1,
    type=str,
    required=True,
    help="Folder where skeletons are stored",
)
@corgie_option(
    "--dst_folder",
    nargs=1,
    type=str,
    required=True,
    help="Folder where to store new skeletons",
)
@corgie_optgroup("Misc Parameters")
@corgie_option("--ids", multiple=True, type=int, help="Ids to transform")
@corgie_option("--ids_filepath", type=str, help="File containing ids to transform")
@corgie_option(
    "--calculate_skeleton_lengths",
    type=bool,
    default=True,
    help="If True, write file containing lengths of each skeleton.",
)
@click.pass_context
def filter_skeletons(
    ctx,
    src_folder,
    dst_folder,
    ids,
    ids_filepath,
    calculate_skeleton_lengths,
):
    scheduler = ctx.obj["scheduler"]

    corgie_logger.debug("Setting up layers...")

    skeleton_ids = ids
    if ids_filepath is not None:
        skeleton_ids = []
        with open(ids_filepath, "r") as f:
            line = f.readline()
            while line:
                skeleton_ids.append(int(line))
                line = f.readline()

    if len(skeleton_ids) == 0:
        skeleton_ids = None
    else:
        skeleton_ids = list(skeleton_ids)

    skeleton_length_file = None
    if calculate_skeleton_lengths:
        import time

        if not os.path.exists("skeleton_lengths"):
            os.makedirs("skeleton_lengths")
        skeleton_length_file = f"skeleton_lengths/skeleton_lengths_{int(time.time())}"

    transform_skeletons_job = FilterSkeletonsJob(
        src_path=src_folder,
        dst_path=dst_folder,
        skeleton_ids=skeleton_ids,
        skeleton_length_file=skeleton_length_file,
    )

    scheduler.register_job(
        transform_skeletons_job,
        job_name="Filtering skeletons in {}".format(src_folder),
    )

    scheduler.execute_until_completion()
    result_report = f"Transformed skeletons stored at {dst_folder}. "
    corgie_logger.info(result_report)

